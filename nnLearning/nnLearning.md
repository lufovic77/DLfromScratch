# 학습 
학습이란, 주어진 데이터를 이용해서 최적의 매개변수 값을 찾아가는 과정이다.  
__최적__ 의 의미는?   
Loss Function를 최소화 하는 값이다.  
이에 관해서는 아래서 알아봄.  
간단하게 이해하면, 손실이 적으면 정확도는 높을 것.  
## 데이터
앞에서 정리한 퍼셉트론의 경우 가중치 값을 사람이 직접 정함.   
지금부터는, 주어진 training data를 통해 스스로 정해간다.   
- Training Data  
- Test Data   

우선, 훈련 데이터를 이용해 가중치를 수정해 나가고,  
얼마나 잘 학습한지를 테스트 데이터를 이용해 평가한다.   
다만, 특정 훈련 데이터 셋에 너무 최적화되어 다른 데이터 셋은 못 맞추는  
현상이 발생 가능하며, 이를 __오버피팅(Overfitting)__ 이라고 한다.  
# Loss Function(Cost Function)
일반적으로 사용하는 함수는 __오차제곱합__ 그리고 __교차 엔트로피 오차__ 이다.   
## SSE(Sum of Squares for Error, 오차제곱합)
말 그대로 오차를 제곱한 값들의 합이다.   
오차: 신경망의 추정값과 실제 답의 차이  
식으로 나타내면  
```python
#y는 신경망의 추정 값 
#t는 실제 정답 레이블
E = (0.5)*SUMk(Yk-Tk)^2
```
python으로 구현하면   
```python
def SSE(y, t):	#y and t are the numpy array
	return (0.5)*np.sum((y-t)**2)
```
ex)   
y = [0.1, 0.1, 0.7, 0.1]    
t = [0,   0,   1,   0]     
신경망은 2번 뉴런을 답으로 추정하고,   
실제로 답이 2번 뉴런인 경우 값은 0.060000000001로 매우 작다.    
즉, 신경망의 정확도가 높다는 의미! (가중치가 잘 학습됨) 
## CEE(Cross Entropy error, 교차 엔트로피 오차)
식은 아래와 같다.    
```python
E = -SUMk(Tk*logYk)
```
식은 복잡해보여도, 결국 T는 정답 레이블 하나 제외하곤 다 0이므로   
정답으로 추정되는 Y의 로그값을 의미한다.   
     
이처럼 정답만 1이고 나머지는 0으로 만드는 것을 원-핫 인코딩이라고 한다.   
다만 주의할 점은 log안의 값이 0이면 -inf로 발산하므로   
이를 방지하기 위해 로그 안에 작은 값을 더해야 한다.   
그래서 python으로 구현하면 다음과 같다.   
```python
def CCE(y, k):
	delta = 1e-7
	return -np.sum(t*np.log(y+delta))
```

