# Neural Network
## 신경망이란?
Input layer, Hidden layer 그리고 Output layer를 가진다.  
중간의 hidden layer이 추가된 것이 퍼셉트론과의 차이점.  

# Activation function
말그래도 입력 신호들에 가중치를 곱해 더한 값이 활성화를 일으키는지 여부를 결정한다.  
사실 퍼셉트론에서도 존재하며, 두 단계로 분리한다 생각하면 된다.  
예를 들어, 퍼셉트론에서는 주로 아래와 같은 식을 사용했다. 
```
	0 (w1x1+w2x2+b <= 0)
y =
	1 (w1x1+w2x2+b > 0)
```
이를 두개로 분리하면, 
```
a = w1x1+w2x2+b,
y = h(a),
		1 (x<=0)
h(x) = 
		0 (x>0)
```
위와 같이 나눌 수 있다.   
여기서, h(x)가 바로 활성화 함수, activation function이다.  
다른 활성화 함수를 퍼셉트론에서 사용하면 바로 신경망으로 나아갈 수 있다. 

## Sigmoid function
가장 자주 사용하는 활성화 함수인 시그모이드 함수이다.  
파이썬으로 구현하면, 
```python
def sigmoid(x):
	y = 1 / (1 + np.exp(-x))
```
위의 함수는 파라미터가 넘파이 배열이여도 문제없이 돌아간다(브로드캐스팅).  
시그모이드 함수를 실제로 그려보면 0에서 1사이의 값을 가지는 S자 모양의 함수가 나온다.  

## Step function 
단층 퍼셉트론에서 사용한 함수로, 임계값을 기준으로 출력이 바뀌는 함수.  
함수 모양이 계단 처럼 생겨서 step function이다. 

## Sigmoid vs. Step
__공통점__
- 입력이 클수록 출력은 1에 가깝다 (vice versa)  
- 출력 값은 항상 0과 1 사이다.  
- 둘 모두 비선형 함수이다.   

__차이점__
- 시그모이드는 매끈한 반면, 계단 함수는 끊겨 있다. 
- 시그모이드는 실수를 반환하지만, 계단 함수는 0 또는 1만 반환한다. 

## 비선형 함수
선형 함수는 y=ax+b같은 형태로 표현될 수 있는 직선 함수이다.  
비선형 함수는 선형함수가 아닌 함수인데, 활성화함수로 반드시 비선형 함수를 사용해야 하는 이유가 이이다.  
만약 선형함수를 활성화 함수로 이용해 다층 신경망을 만든다면,  
아무리 깊게 쌓아도 결국에는 다시 선형 함수에 불과하다.  
즉, 단층 신경망으로도 표현할 수 있기에 의미가 없다!  
ex)
```
h(x) = 3x
y(x) = h(h(h(x))) = 27x^3 = bx
```
결국 다시 선형함수로 표현할 수 있게 된다.  

# 배치 처리
배치 처리란 입력 데이터를 모아서 한번에 입력으로 넘기는 방법이다.     
유명한 MNIST 데이터 셋 추론 과정을 한번 보자.  
숫자 0~9의 손글시 사진을 주고, 실제 숫자가 무엇인지 맞추는 문제.  
## 조건     
- 한 이미지가 28x28(784)개의 픽셀로 이루어짐.  
- Hidden Layer는 3층이다.  
- 10개의 숫자 이므로 출력층에는 10갱의 뉴런이 존재한다.  
## 형상
- 이미지 1장만 입력하는 케이스

(Input)784 -> (Hidden1)784x50 -> (Hidden2)50x100 -> (Hidden3)100x10 -> (Output)10    
- 이미지 100장을 입력하는 케이스   

(Input)100x784 -> (Hidden1)784x50 -> (Hidden2)50x100 -> (Hidden3)100x10 -> (Output)100x10      
     
2번의 경우는 입력의 형상이 100x784이고 결과적으로 출력이 100x10임에 주목하자.   
## 왜 배치 처리를 하나?
- 존재하는 수치 라이브러리들은 큰 배열을 효율적으로 처리하게 최적화됨.   
- 데이터 전송이 bottleneck으로 작용. 차라리 한번에 보내는게 작은걸 여러 번 보내는 것보다 빠르다.  

# 신경망의 출력층(Output Layer)
신경망은  
Input Layer -> Hidden Layer -> Output Layer  
순으로 구성이 되며, 이중 출력층에 대한 설명이다.  

Machine Learning의 문제는 classification, regression으로 나뉜다.  
- Classification: 특정 클래스 소속 여부 결정  
- Regression: 데이터를 보고 연속적인 수치를 predict    

여기서 classification은 출력층의 함수로 softmax함수를 사용하며,  
regression은 항등함수(입력값이 곧 출력값)를 사용한다.   
해당 이유에 대해서는 뒤에 설명된다.  
# Softmax
- n: 출력층의 뉴런 수.  
- yk: 그 중 k번 뉴런.  
 - exp: 시그모이드에서 사용한 그 함수. 자연상수의 지수승.   
 - ak: 입력신호.   
 - sum: 1부터 n 시그마   
 ```
 yk = exp(ak)/sum(exp(a))
 ```
그냥 이게 소프트 맥스 함수다.    
다음과 같이 간단하게 구현 가능하다.  
```python
def softmax(a): #numpy array a
	exp_a = np.exp(a)
	sum = np.sum(exp_a)
	return exp_a / sum
```
식을 보면 알듯이, 
분모가 입력층(출력층 이전의 은닉층)의 합이므로,  
소프트맥스 함수는 모든 입력신호에 대해 영향을 받음을 파악 가능.  
## 왜 Classification에서는 소프트맥스 함수를?
앞서 언급한 것처럼 분류 문제에서는 출력층에 소프트맥스 함수를 사용한다.  
이유는 __소프트맥스의 출력값들의 합이 1이므로!!__  
즉, 각 출력값을 곧 확률로 보면 된다.  
클래스 소속 여부를 결정하는 문제가 분류이므로,   
출력값을 가지고 몇%의 확률로 해당 클래스에 소속되는지 보여주면 된다.   
일반적으로는 가장 큰 출력값을 가지는 뉴런을 클래스로 분류한다.  

다만, 소프트맥스 함수는 입력값들의 대소 여부에는 영향이 없어서  
종종 inference에서는  지수함수 계산을 생략하기 위해 소프트맥스를 생략하기도 한다.   

학습과정에서는 출력층에 소프트맥스 함수를 넣고,  
추론과정에서는 생략하는 것이 일반적.  
## Softmax 구현 시 주의점
소프트맥스는 구현 상에서 지수함수를 도입하므로  
중간값들이 매우 커져 오버플로가 발생 가능하다.  
식에서 임의의 정수를 도입하면,  
결국 임의의 정수를 더하거나 빼도 결과에는 영향이 없다.  

__입력값의 최댓값을 입력신호들에서 빼주면__   
결과값 영향없이 오버플로우를 방지할 수 있다. 

해당 코드는 softmax.py에 정리해 두겠다. 



# Reference
밑바닥부터 시작하는 딥러닝(Deep Learning from Scratch)
