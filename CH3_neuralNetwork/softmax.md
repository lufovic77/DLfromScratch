# 신경망의 출력층(Output Layer)
신경망은  
Input Layer -> Hidden Layer -> Output Layer  
순으로 구성이 되며, 이중 출력층에 대한 설명이다.  

Machine Learning의 문제는 classification, regression으로 나뉜다.  
- Classification: 특정 클래스 소속 여부 결정  
- Regression: 데이터를 보고 연속적인 수치를 predict    

여기서 classification은 출력층의 함수로 softmax함수를 사용하며,  
regression은 항등함수(입력값이 곧 출력값)를 사용한다.   
해당 이유에 대해서는 뒤에 설명된다.  
# Softmax
- n: 출력층의 뉴런 수.  
- yk: 그 중 k번 뉴런.  
 - exp: 시그모이드에서 사용한 그 함수. 자연상수의 지수승.   
 - ak: 입력신호.   
 - sum: 1부터 n 시그마   
 ```
 yk = exp(ak)/sum(exp(a))
 ```
그냥 이게 소프트 맥스 함수다.    
다음과 같이 간단하게 구현 가능하다.  
```python
def softmax(a): #numpy array a
	exp_a = np.exp(a)
	sum = np.sum(exp_a)
	return exp_a / sum
```
식을 보면 알듯이, 
분모가 입력층(출력층 이전의 은닉층)의 합이므로,  
소프트맥스 함수는 모든 입력신호에 대해 영향을 받음을 파악 가능.  
## 왜 Classification에서는 소프트맥스 함수를?
앞서 언급한 것처럼 분류 문제에서는 출력층에 소프트맥스 함수를 사용한다.  
이유는 __소프트맥스의 출력값들의 합이 1이므로!!__  
즉, 각 출력값을 곧 확률로 보면 된다.  
클래스 소속 여부를 결정하는 문제가 분류이므로,   
출력값을 가지고 몇%의 확률로 해당 클래스에 소속되는지 보여주면 된다.   
일반적으로는 가장 큰 출력값을 가지는 뉴런을 클래스로 분류한다.  

다만, 소프트맥스 함수는 입력값들의 대소 여부에는 영향이 없어서  
종종 inference에서는  지수함수 계산을 생략하기 위해 소프트맥스를 생략하기도 한다.   

학습과정에서는 출력층에 소프트맥스 함수를 넣고,  
추론과정에서는 생략하는 것이 일반적.  
## Softmax 구현 시 주의점
소프트맥스는 구현 상에서 지수함수를 도입하므로  
중간값들이 매우 커져 오버플로가 발생 가능하다.  
식에서 임의의 정수를 도입하면,  
결국 임의의 정수를 더하거나 빼도 결과에는 영향이 없다.  

__입력값의 최댓값을 입력신호들에서 빼주면__   
결과값 영향없이 오버플로우를 방지할 수 있다. 

해당 코드는 softmax.py에 정리해 두겠다. 
